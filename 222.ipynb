{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T02:16:09.019647Z",
     "start_time": "2025-09-08T02:15:56.663045Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import os\n",
    "model_id='Qwen/Qwen3-0.6B'\n",
    "hf_token=\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side='right'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:20:35.986976Z",
     "start_time": "2025-09-08T02:20:35.966476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ],
   "id": "40a2530289a09d96",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:20:45.351309Z",
     "start_time": "2025-09-08T02:20:37.772196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=5,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    token=hf_token  # 토큰 추가\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "e2a97f89ea7b0aae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:20:49.228393Z",
     "start_time": "2025-09-08T02:20:47.110055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 엑셀 데이터 로드\n",
    "# -------------------------------\n",
    "\n",
    "# 엑셀 파일 경로 (수정 필요)\n",
    "train_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/train.xlsx\"  # 200행\n",
    "test_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/test.xlsx\"    # 50행\n",
    "\n",
    "# 엑셀 읽기\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "print(f\"Train 데이터 크기: {train_df.shape}\")\n",
    "print(f\"Test 데이터 크기: {test_df.shape}\")\n",
    "print(f\"칼럼: {list(train_df.columns)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 텍스트 칼럼들을 합치기\n",
    "# -------------------------------\n",
    "\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    \"\"\"4개 텍스트 칼럼을 하나로 합치기\"\"\"\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():  # null이 아니고 빈 문자열이 아닌 경우\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "# 텍스트 칼럼 합치기\n",
    "train_df['text'] = train_df.apply(combine_text_columns, axis=1)\n",
    "test_df['text'] = test_df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "print(f\"\\n합쳐진 텍스트 예시:\")\n",
    "print(train_df['text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 라벨 인코딩\n",
    "# -------------------------------\n",
    "\n",
    "# 사용자태그를 라벨로 변환\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train + test 전체 라벨로 인코더 학습 (일관성 유지)\n",
    "all_labels = pd.concat([train_df['사용자태그'], test_df['사용자태그']])\n",
    "label_encoder.fit(all_labels.dropna())\n",
    "\n",
    "# 라벨 인코딩 적용\n",
    "train_df['label'] = label_encoder.transform(train_df['사용자태그'])\n",
    "test_df['label'] = label_encoder.transform(test_df['사용자태그'])\n",
    "\n",
    "# 라벨 정보 출력\n",
    "unique_labels = label_encoder.classes_\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"\\n라벨 정보:\")\n",
    "print(f\"총 라벨 수: {num_labels}\")\n",
    "for i, label in enumerate(unique_labels):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "print(f\"\\nTrain 라벨 분포:\")\n",
    "print(train_df['사용자태그'].value_counts())\n",
    "print(f\"\\nTest 라벨 분포:\")\n",
    "print(test_df['사용자태그'].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 청크 함수 정의\n",
    "# -------------------------------\n",
    "\n",
    "def create_chunked_dataset(df, tokenizer, max_length=512, stride=50):\n",
    "    \"\"\"데이터프레임을 청크 데이터셋으로 변환\"\"\"\n",
    "    chunked_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        chunks = chunk_text(row['text'], tokenizer, max_length, stride)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_row = {\n",
    "                'text': chunk,\n",
    "                'label': row['label'],\n",
    "                'patent_id': row.get('출원번호', f'patent_{len(chunked_rows)}')\n",
    "            }\n",
    "            chunked_rows.append(chunk_row)\n",
    "\n",
    "    return pd.DataFrame(chunked_rows)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 청크 함수 및 청크 데이터셋 생성\n",
    "# -------------------------------\n",
    "def chunk_text(text, max_length=400, stride=50):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunks.append(tokenizer.decode(tokens[start:end], skip_special_tokens=True))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start += max_length - stride\n",
    "    return chunks\n",
    "\n",
    "# 원본 데이터를 청크로 분할\n",
    "train_chunks = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    chunks = chunk_text(row['text'])\n",
    "    for chunk in chunks:\n",
    "        train_chunks.append({'text': chunk, 'label': row['label'], 'patent_id': idx})\n",
    "\n",
    "test_chunks = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    chunks = chunk_text(row['text'])\n",
    "    for chunk in chunks:\n",
    "        test_chunks.append({'text': chunk, 'label': row['label'], 'patent_id': idx})\n",
    "\n",
    "print(f\"청크 전 - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(f\"청크 후 - Train: {len(train_chunks)}, Test: {len(test_chunks)}\")\n",
    "\n",
    "# Dataset 객체 생성 (청크 데이터 사용)\n",
    "train_dataset_dict = {\n",
    "    'text': [chunk['text'] for chunk in train_chunks],\n",
    "    'label': [chunk['label'] for chunk in train_chunks]\n",
    "}\n",
    "\n",
    "test_dataset_dict = {\n",
    "    'text': [chunk['text'] for chunk in test_chunks],\n",
    "    'label': [chunk['label'] for chunk in test_chunks]\n",
    "}\n",
    "\n",
    "# 추론용으로 청크 정보 저장\n",
    "train_chunk_info = pd.DataFrame(train_chunks)\n",
    "test_chunk_info = pd.DataFrame(test_chunks)\n",
    "\n",
    "train_data = Dataset.from_dict(train_dataset_dict)\n",
    "test_data = Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset 생성 완료:\")\n",
    "print(f\"Train: {len(dataset['train'])}개 청크\")\n",
    "print(f\"Test: {len(dataset['test'])}개 청크\")"
   ],
   "id": "df26a1375d618bff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 크기: (200, 9)\n",
      "Test 데이터 크기: (50, 9)\n",
      "칼럼: ['출원번호', 'DB종류', '특허/실용 구분', '발명의 명칭', '요약', '전체청구항', '대표청구항', '사용자태그', 'WINTELIPS KEY']\n",
      "\n",
      "합쳐진 텍스트 예시:\n",
      "발명의 명칭: Method of producing zeolite film | 요약: Provided is a method of producing a zeolite film continuously and efficiently. The method of forming zeolite on a surface of a support is characterized i...\n",
      "\n",
      "라벨 정보:\n",
      "총 라벨 수: 5\n",
      "  0: CPC_C01B\n",
      "  1: CPC_C01C\n",
      "  2: CPC_C01D\n",
      "  3: CPC_C01F\n",
      "  4: CPC_C01G\n",
      "\n",
      "Train 라벨 분포:\n",
      "사용자태그\n",
      "CPC_C01B    40\n",
      "CPC_C01C    40\n",
      "CPC_C01D    40\n",
      "CPC_C01F    40\n",
      "CPC_C01G    40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test 라벨 분포:\n",
      "사용자태그\n",
      "CPC_C01B    10\n",
      "CPC_C01C    10\n",
      "CPC_C01D    10\n",
      "CPC_C01F    10\n",
      "CPC_C01G    10\n",
      "Name: count, dtype: int64\n",
      "청크 전 - Train: 200, Test: 50\n",
      "청크 후 - Train: 903, Test: 207\n",
      "\n",
      "Dataset 생성 완료:\n",
      "Train: 903개 청크\n",
      "Test: 207개 청크\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:20:59.434714Z",
     "start_time": "2025-09-08T02:20:58.692982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "    tokenized['labels'] = examples['label']\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n",
    "tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(['text', 'label'])\n",
    "tokenized_test = tokenized_test.remove_columns(['text', 'label'])\n",
    "\n",
    "print(f\"\\n토크나이징 완료:\")\n",
    "print(tokenized_train[0])"
   ],
   "id": "ad3f54cb12d26844",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bda224f7b82d443b9dcdf95901d7989e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e33f92114f7c439f891012f8ab69808b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "토크나이징 완료:\n",
      "{'input_ids': [126835, 79632, 20401, 130345, 141676, 25, 6730, 315, 17387, 13703, 337, 632, 4531, 760, 85997, 125535, 25, 53874, 374, 264, 1714, 315, 17387, 264, 13703, 337, 632, 4531, 30878, 323, 29720, 13, 576, 1714, 315, 29064, 13703, 337, 632, 389, 264, 7329, 315, 264, 1824, 374, 31871, 304, 429, 279, 1714, 5646, 25, 264, 1156, 3019, 315, 71808, 13703, 337, 632, 6915, 47373, 311, 264, 7329, 315, 264, 1824, 26, 264, 2086, 3019, 315, 20045, 27268, 17837, 369, 7826, 279, 6915, 47373, 26, 264, 4843, 3019, 315, 10687, 279, 1824, 323, 279, 27268, 17837, 1119, 264, 19259, 37629, 323, 16380, 16643, 68118, 38875, 26, 323, 264, 11737, 3019, 315, 15826, 279, 1824, 389, 892, 13703, 337, 632, 702, 1012, 16643, 696, 76, 745, 91006, 11, 323, 304, 279, 4843, 3019, 11, 279, 9315, 11, 7262, 11, 323, 6396, 315, 279, 27268, 17837, 304, 279, 19259, 37629, 374, 23368, 11, 279, 1824, 374, 7726, 1660, 77208, 304, 279, 27268, 17837, 11, 279, 12720, 882, 315, 279, 16643, 68118, 38875, 374, 23368, 553, 42368, 279, 882, 504, 979, 279, 1824, 28833, 279, 19259, 37629, 311, 979, 279, 1824, 42086, 279, 19259, 37629, 13, 760, 137138, 125118, 88259, 126524, 25, 220, 16, 13, 362, 1714, 315, 17387, 264, 13703, 337, 632, 4531, 553, 29064, 13703, 337, 632, 389, 264, 7329, 315, 264, 1824, 11, 31871, 304, 429, 279, 1714, 39995, 279, 2701, 1156, 311, 11737, 7354, 25, 264, 1156, 3019, 315, 71808, 13703, 337, 632, 6915, 47373, 311, 264, 7329, 315, 264, 1824, 48222, 2086, 3019, 315, 20045, 27268, 17837, 369, 7826, 279, 6915, 47373, 48222, 4843, 3019, 315, 10687, 279, 27268, 17837, 1119, 264, 19259, 37629, 323, 16380, 16643, 68118, 38875, 315, 279, 13703, 337, 632, 389, 279, 7329, 315, 279, 1824, 1393, 7218, 279, 1824, 304, 279, 19259, 37629, 26, 44001, 11737, 3019, 315, 15826, 279, 1824, 389, 892, 13703, 337, 632, 702, 1012, 16643, 696, 76, 745, 91006, 11, 41318, 258, 279, 4843, 3019, 11, 279, 9315, 11, 7262, 11, 323, 6396, 315, 279, 27268, 17837, 304, 279, 19259, 37629, 374, 23368, 11, 279, 1824, 374, 7726, 1660, 77208, 304, 279, 27268, 17837, 11, 279, 12720, 882, 315, 279, 16643, 68118, 38875, 374, 23368, 553, 42368, 279, 882, 504, 979, 279, 1824, 28833, 279, 19259, 37629, 311, 979, 279, 1824, 42086, 279, 19259, 37629, 13, 760, 220, 17, 13, 576, 1714, 315, 17387, 264, 13703, 337, 632, 4531, 4092, 311], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:21:04.783489Z",
     "start_time": "2025-09-08T02:21:04.750443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "def compute_metrics(pred):\n",
    "    labels=pred.label_ids\n",
    "    preds=pred.predictions.argmax(-1)\n",
    "    precision, recall, f1,_ =precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc=accuracy_score(labels,preds)\n",
    "    print(\"\\n Classification Report\")\n",
    "    print(classification_report(labels, preds, digits=2))\n",
    "    logits_tensor=torch.tensor(pred.predictions)\n",
    "    labels_tensor=torch.tensor(pred.label_ids)\n",
    "    loss=F.cross_entropy(logits_tensor,labels_tensor).item()\n",
    "    return{\n",
    "        'accuracy':acc,\n",
    "        'f1':f1,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'eval_loss':loss\n",
    "    }"
   ],
   "id": "399cf6ec3e08ea77",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:21:09.274188Z",
     "start_time": "2025-09-08T02:21:07.062370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "peft_config=LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS',\n",
    "    target_modules=['k_proj','gate_proj','v_proj','up_proj','q_proj','o_proj','down_proj']\n",
    ")\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "model=get_peft_model(model,peft_config)"
   ],
   "id": "1485c0a38efe0bba",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T02:21:50.744792Z",
     "start_time": "2025-09-08T02:21:50.728630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "output_dir= \"../output\"\n",
    "training_arguments=SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim='paged_adamw_32bit',\n",
    "    lr_scheduler_type='cosine',\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field='text',\n",
    "    max_length=512,\n",
    "    label_names=['labels']\n",
    ")"
   ],
   "id": "ea24041b2ebdd085",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:47:52.880754Z",
     "start_time": "2025-09-08T02:21:52.561879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from trl import SFTTrainer\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "trainer.model.save_pretrained('Qwen3-0.6B-QLoRA2')"
   ],
   "id": "591d60b323e438d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "283ed6c19cd74a01abbff4c4bb61f317"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc1fb9fc882345fb9eb675231780eafc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='452' max='452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [452/452 1:22:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.093300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.274700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "53f45a927d1d88bc61284aa357765c9d"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/104 03:06]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "6dc9753e962dc7e7c30e94867720b700"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.48      0.48        50\n",
      "           1       0.73      0.73      0.73        37\n",
      "           2       0.82      0.62      0.71        37\n",
      "           3       0.52      0.82      0.64        45\n",
      "           4       0.55      0.29      0.38        38\n",
      "\n",
      "    accuracy                           0.59       207\n",
      "   macro avg       0.62      0.59      0.59       207\n",
      "weighted avg       0.61      0.59      0.58       207\n",
      "\n",
      "{'eval_loss': 1.1379822492599487, 'eval_accuracy': 0.5893719806763285, 'eval_f1': 0.5859821882969716, 'eval_precision': 0.6185746594031597, 'eval_recall': 0.58860945155682, 'eval_runtime': 188.3527, 'eval_samples_per_second': 1.099, 'eval_steps_per_second': 0.552}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:48:23.754830Z",
     "start_time": "2025-09-08T03:48:07.751276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------------\n",
    "# QLoRA SEQ_CLS 어댑터 머지\n",
    "# -------------------------------\n",
    "\n",
    "print(\"=== QLoRA Adapter Merge ===\")\n",
    "\n",
    "# 1. 베이스 모델을 SEQ_CLS로 직접 로드 (어댑터 훈련 시와 동일)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    num_labels=5,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "print(\"Base model structure:\")\n",
    "for name, _ in base_model.named_modules():\n",
    "    if 'score' in name or 'classifier' in name:\n",
    "        print(f\"  Found: {name}\")\n",
    "\n",
    "# 2. 어댑터 로드 및 머지\n",
    "adapter_path = \"/Qwen3-0.6B-QLoRA\"\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    ignore_mismatched_sizes=False,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# 3. 머지 및 저장\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"C:/wips/output/Qwen3_merged_method3\")\n",
    "print(\"머지 완료!\")"
   ],
   "id": "ec31f5f8e0de2c24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QLoRA Adapter Merge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model structure:\n",
      "  Found: score\n",
      "머지 완료!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T04:45:04.530237Z",
     "start_time": "2025-09-08T04:32:51.820692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"머지 완료!\")\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드 (기존 코드 그대로)\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 엑셀 데이터 로드 및 전처리\n",
    "# -------------------------------\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "train_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/train.xlsx\"  # 200행\n",
    "test_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/test.xlsx\"  # 50행\n",
    "\n",
    "# 엑셀 읽기\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "# 텍스트 칼럼들을 합치기\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "\n",
    "train_df['text'] = train_df.apply(combine_text_columns, axis=1)\n",
    "test_df['text'] = test_df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([train_df['사용자태그'], test_df['사용자태그']])\n",
    "label_encoder.fit(all_labels.dropna())\n",
    "\n",
    "train_df['label'] = label_encoder.transform(train_df['사용자태그'])\n",
    "test_df['label'] = label_encoder.transform(test_df['사용자태그'])\n",
    "\n",
    "# Dataset 객체로 변환\n",
    "train_data = Dataset.from_dict({\n",
    "    'text': train_df['text'].tolist(),\n",
    "    'label': train_df['label'].tolist()\n",
    "})\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    'text': test_df['text'].tolist(),\n",
    "    'label': test_df['label'].tolist()\n",
    "})\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 기존 방식과 동일한 샘플링\n",
    "# -------------------------------\n",
    "\n",
    "# 기존 코드와 동일한 구조\n",
    "ds = DatasetDict({'train': train_data, 'test': test_data})\n",
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]\n",
    "\n",
    "num_labels = len(set(train[\"label\"]))\n",
    "train_per_label = 200 // num_labels  # 전체 200개를 라벨별로 균등 분배\n",
    "test_per_label = 50 // num_labels  # 전체 50개를 라벨별로 균등 분배\n",
    "\n",
    "print(f\"라벨 수: {num_labels}\")\n",
    "print(f\"라벨별 train 샘플: {train_per_label}개\")\n",
    "print(f\"라벨별 test 샘플: {test_per_label}개\")\n",
    "\n",
    "\n",
    "def sample_per_label(dataset, per_label, seed=42):\n",
    "    labels = np.array(dataset[\"label\"])\n",
    "    picked = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for l in range(num_labels):\n",
    "        idx = np.where(labels == l)[0]\n",
    "        if len(idx) < per_label:\n",
    "            print(f\"경고: 라벨 {l}에 {len(idx)}개 샘플만 있음 (필요: {per_label}개)\")\n",
    "            picked.extend(idx)  # 있는 만큼만 추가\n",
    "        else:\n",
    "            picked.extend(rng.choice(idx, per_label, replace=False))\n",
    "    return dataset.select(sorted(picked))\n",
    "\n",
    "\n",
    "train_small = sample_per_label(train, train_per_label)\n",
    "test_small = sample_per_label(test, test_per_label)\n",
    "\n",
    "# 라벨 분포 확인\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"\\nTrain 라벨 분포: {Counter(train_small['label'])}\")\n",
    "print(f\"Test 라벨 분포: {Counter(test_small['label'])}\")\n",
    "\n",
    "# 원본 라벨로 변환해서 확인\n",
    "train_original_labels = [label_encoder.inverse_transform([label])[0] for label in train_small['label']]\n",
    "test_original_labels = [label_encoder.inverse_transform([label])[0] for label in test_small['label']]\n",
    "\n",
    "print(f\"\\nTrain 원본 라벨 분포: {Counter(train_original_labels)}\")\n",
    "print(f\"Test 원본 라벨 분포: {Counter(test_original_labels)}\")\n",
    "\n",
    "# CUDA 및 디바이스 확인\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"현재 GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"토크나이저 로딩 완료!\")\n",
    "\n",
    "# 모델 로드 (수정된 부분)\n",
    "print(\"모델 로딩 중...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    r\"/output/Qwen3_merged_method2\",\n",
    "    num_labels=num_labels,  # 동적으로 계산된 라벨 수 사용\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # 자동 디바이스 배치만 사용\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"모델 로딩 완료!\")\n",
    "\n",
    "# pad_token_id 설정\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"모델 디바이스: {next(model.parameters()).device}\")\n",
    "print(f\"모델 dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 청크 함수 정의 및 청크 생성\n",
    "# -------------------------------\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=400, stride=50):\n",
    "    \"\"\"텍스트를 청크로 분할하는 함수\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunks.append(tokenizer.decode(tokens[start:end], skip_special_tokens=True))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start += max_length - stride\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "print(\"\\n청크 데이터 생성 중...\")\n",
    "\n",
    "# Train 데이터 청크 생성\n",
    "train_chunks = []\n",
    "train_patent_info = []\n",
    "for idx in range(len(train_small)):\n",
    "    text = train_small[idx]['text']\n",
    "    label = train_small[idx]['label']\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "\n",
    "    # 각 청크를 저장\n",
    "    for chunk in chunks:\n",
    "        train_chunks.append({\n",
    "            'text': chunk,\n",
    "            'label': label,\n",
    "            'patent_id': idx,\n",
    "            'chunk_len': len(chunk.split())\n",
    "        })\n",
    "\n",
    "    # 특허별 정보 저장\n",
    "    train_patent_info.append({\n",
    "        'patent_id': idx,\n",
    "        'original_text': text,\n",
    "        'label': label,\n",
    "        'chunk_count': len(chunks)\n",
    "    })\n",
    "\n",
    "# Test 데이터 청크 생성\n",
    "test_chunks = []\n",
    "test_patent_info = []\n",
    "for idx in range(len(test_small)):\n",
    "    text = test_small[idx]['text']\n",
    "    label = test_small[idx]['label']\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "\n",
    "    # 각 청크를 저장 (patent_id는 train 이후 번호로)\n",
    "    for chunk in chunks:\n",
    "        test_chunks.append({\n",
    "            'text': chunk,\n",
    "            'label': label,\n",
    "            'patent_id': idx + len(train_small),  # train 이후 번호\n",
    "            'chunk_len': len(chunk.split())\n",
    "        })\n",
    "\n",
    "    # 특허별 정보 저장\n",
    "    test_patent_info.append({\n",
    "        'patent_id': idx + len(train_small),\n",
    "        'original_text': text,\n",
    "        'label': label,\n",
    "        'chunk_count': len(chunks)\n",
    "    })\n",
    "\n",
    "print(f\"Train: {len(train_small)}개 특허 → {len(train_chunks)}개 청크\")\n",
    "print(f\"Test: {len(test_small)}개 특허 → {len(test_chunks)}개 청크\")\n",
    "\n",
    "# 전체 청크 데이터\n",
    "all_chunks = train_chunks + test_chunks\n",
    "all_chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
    "\n",
    "print(f\"전체 청크 수: {len(all_chunks)}개\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 청크별 예측 수행\n",
    "# -------------------------------\n",
    "\n",
    "model.eval()\n",
    "chunk_predictions = []\n",
    "\n",
    "print(\"\\n청크별 예측 수행 중...\")\n",
    "batch_size = 8  # 배치 사이즈를 변수로 분리\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(all_chunk_texts), batch_size):\n",
    "        batch_texts = all_chunk_texts[i:i + batch_size]\n",
    "\n",
    "        # 토크나이저 처리 시 디바이스 확인\n",
    "        inputs = tokenizer(batch_texts, truncation=True, padding=True,\n",
    "                           max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        # 입력을 모델과 같은 디바이스로 이동\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        chunk_predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "        # 진행 상황 출력\n",
    "        if (i // batch_size + 1) % 50 == 0:\n",
    "            print(f\"진행 중: {i + len(batch_texts)}/{len(all_chunk_texts)} 청크 완료\")\n",
    "\n",
    "print(f\"총 {len(chunk_predictions)}개 청크 예측 완료\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. 특허별 예측 집계 (가중 평균)\n",
    "# -------------------------------\n",
    "\n",
    "def aggregate_patent_predictions(chunks, chunk_preds, start_idx=0):\n",
    "    \"\"\"특허별로 청크 예측을 가중 평균으로 집계\"\"\"\n",
    "    patent_preds = []\n",
    "    patent_labels = []\n",
    "\n",
    "    # 특허별로 그룹화\n",
    "    patent_groups = {}\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        patent_id = chunk['patent_id']\n",
    "        if patent_id not in patent_groups:\n",
    "            patent_groups[patent_id] = []\n",
    "        patent_groups[patent_id].append((i + start_idx, chunk))\n",
    "\n",
    "    # 각 특허별로 가중 평균 계산\n",
    "    for patent_id in sorted(patent_groups.keys()):\n",
    "        chunk_indices_and_info = patent_groups[patent_id]\n",
    "\n",
    "        # 해당 특허의 청크 예측값들과 가중치 수집\n",
    "        patent_chunk_preds = []\n",
    "        weights = []\n",
    "\n",
    "        for chunk_idx, chunk_info in chunk_indices_and_info:\n",
    "            patent_chunk_preds.append(chunk_preds[chunk_idx])\n",
    "            weights.append(chunk_info['chunk_len'])  # 청크 길이를 가중치로 사용\n",
    "\n",
    "        # 가중 평균 계산\n",
    "        patent_chunk_preds = np.array(patent_chunk_preds)\n",
    "        weights = np.array(weights)\n",
    "\n",
    "        weighted_pred = np.average(patent_chunk_preds, axis=0, weights=weights)\n",
    "        final_pred = np.argmax(weighted_pred)\n",
    "\n",
    "        patent_preds.append(final_pred)\n",
    "        patent_labels.append(chunk_indices_and_info[0][1]['label'])  # 모든 청크가 같은 라벨\n",
    "\n",
    "    return patent_preds, patent_labels\n",
    "\n",
    "\n",
    "# Train과 Test 각각 집계\n",
    "print(\"\\n특허별 예측 집계 중...\")\n",
    "train_preds, train_labels = aggregate_patent_predictions(\n",
    "    train_chunks, chunk_predictions, start_idx=0\n",
    ")\n",
    "\n",
    "test_preds, test_labels = aggregate_patent_predictions(\n",
    "    test_chunks, chunk_predictions, start_idx=len(train_chunks)\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. 결과 출력\n",
    "# -------------------------------\n",
    "\n",
    "# 전체 결과\n",
    "all_preds = train_preds + test_preds\n",
    "all_true_labels = train_labels + test_labels\n",
    "\n",
    "total_accuracy = accuracy_score(all_true_labels, all_preds)\n",
    "train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"\\n=== 청크 기반 예측 결과 ===\")\n",
    "print(f\"전체 정확도: {total_accuracy:.4f} ({total_accuracy * 100:.2f}%)\")\n",
    "print(f\"학습 데이터 정확도: {train_accuracy:.4f} ({train_accuracy * 100:.2f}%)\")\n",
    "print(f\"검증 데이터 정확도: {test_accuracy:.4f} ({test_accuracy * 100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n상세 통계:\")\n",
    "print(f\"- 전체 특허: {len(all_true_labels)}개\")\n",
    "print(f\"- 전체 청크: {len(chunk_predictions)}개\")\n",
    "print(f\"- 평균 청크/특허: {len(chunk_predictions) / len(all_true_labels):.1f}개\")\n",
    "\n",
    "# 라벨별 정확도도 확인\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"\\n=== 상세 분류 리포트 ===\")\n",
    "target_names = [label_encoder.inverse_transform([i])[0] for i in range(num_labels)]\n",
    "print(classification_report(all_true_labels, all_preds, target_names=target_names, digits=4))"
   ],
   "id": "2eef77558bdc48b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "머지 완료!\n",
      "라벨 수: 5\n",
      "라벨별 train 샘플: 40개\n",
      "라벨별 test 샘플: 10개\n",
      "\n",
      "Train 라벨 분포: Counter({0: 40, 1: 40, 2: 40, 3: 40, 4: 40})\n",
      "Test 라벨 분포: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10})\n",
      "\n",
      "Train 원본 라벨 분포: Counter({'CPC_C01B': 40, 'CPC_C01C': 40, 'CPC_C01D': 40, 'CPC_C01F': 40, 'CPC_C01G': 40})\n",
      "Test 원본 라벨 분포: Counter({'CPC_C01B': 10, 'CPC_C01C': 10, 'CPC_C01D': 10, 'CPC_C01F': 10, 'CPC_C01G': 10})\n",
      "CUDA 사용 가능: True\n",
      "GPU 이름: NVIDIA GeForce GTX 1660 Ti\n",
      "GPU 메모리: 6.00 GB\n",
      "현재 GPU 메모리 사용량: 0.00 GB\n",
      "토크나이저 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로딩 완료!\n",
      "모델 로딩 중...\n",
      "모델 로딩 완료!\n",
      "모델 디바이스: cuda:0\n",
      "모델 dtype: torch.float16\n",
      "GPU 메모리 사용량: 1.11 GB\n",
      "\n",
      "청크 데이터 생성 중...\n",
      "Train: 200개 특허 → 903개 청크\n",
      "Test: 50개 특허 → 207개 청크\n",
      "전체 청크 수: 1110개\n",
      "\n",
      "청크별 예측 수행 중...\n",
      "진행 중: 400/1110 청크 완료\n",
      "진행 중: 800/1110 청크 완료\n",
      "총 1110개 청크 예측 완료\n",
      "\n",
      "특허별 예측 집계 중...\n",
      "\n",
      "=== 청크 기반 예측 결과 ===\n",
      "전체 정확도: 0.4400 (44.00%)\n",
      "학습 데이터 정확도: 0.4650 (46.50%)\n",
      "검증 데이터 정확도: 0.3400 (34.00%)\n",
      "\n",
      "상세 통계:\n",
      "- 전체 특허: 250개\n",
      "- 전체 청크: 1110개\n",
      "- 평균 청크/특허: 4.4개\n",
      "\n",
      "=== 상세 분류 리포트 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CPC_C01B     0.2963    0.1600    0.2078        50\n",
      "    CPC_C01C     0.6667    0.7200    0.6923        50\n",
      "    CPC_C01D     0.3404    0.6400    0.4444        50\n",
      "    CPC_C01F     0.4444    0.4800    0.4615        50\n",
      "    CPC_C01G     0.4762    0.2000    0.2817        50\n",
      "\n",
      "    accuracy                         0.4400       250\n",
      "   macro avg     0.4448    0.4400    0.4176       250\n",
      "weighted avg     0.4448    0.4400    0.4176       250\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a985abf09a27480",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
