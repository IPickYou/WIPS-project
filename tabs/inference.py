import os
import streamlit as st

def show():
    st.write("### ëª¨ë¸ ì¶”ë¡ ")
    st.write("**ëª¨ë¸ ì„¤ì •**")

    # ëª¨ë¸ ì„¤ì •
    col1, col2 = st.columns(2)
    
    with col1:
        model_id = st.text_input(
            "ë² ì´ìŠ¤ ëª¨ë¸ ID",
            value="meta-llama/Llama-3.2-1B",
            key="model_id"
        )
        
        tokenizer_path = st.text_input(
            "í† í¬ë‚˜ì´ì € ê²½ë¡œ",
            value="C:/aimer/wips/models/wips_t",
            key="tokenizer_path"
        )
    
    with col2:
        hf_token = st.text_input(
            "HuggingFace Token",
            value=os.getenv("HF_TOKEN"),
            type="password",
            key="hf_token"
        )
        
        num_labels = st.number_input(
            "ë¼ë²¨ ìˆ˜",
            min_value=2,
            max_value=100,
            value=5,
            key="num_labels"
        )
    if st.button("ğŸ”„ ëª¨ë¸ ë¡œë“œ"):
        with st.spinner("ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            try:
                import torch
                from transformers import AutoModelForSequenceClassification, AutoTokenizer
                
                # ëª¨ë¸ ë¡œë“œ (ì˜ˆì‹œ ì½”ë“œ ë°©ì‹)
                model = AutoModelForSequenceClassification.from_pretrained(
                    model_id,
                    device_map="cuda",
                    token=hf_token,
                    num_labels=num_labels,
                    offload_folder="C:/aimer/wips/temp_offload"
                )
                
                # ëª¨ë¸ evaluation ëª¨ë“œë¡œ ì„¤ì •
                model.eval()
                
                # í† í¬ë‚˜ì´ì € ë¡œë“œ
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,use_fast=True)
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.padding_side = "right"
                if tokenizer.pad_token_id is None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                model.config.pad_token_id = tokenizer.pad_token_id
                st.session_state.model = model
                st.session_state.tokenizer = tokenizer
                st.session_state.model_loaded = True
                
                st.success("ëª¨ë¸ ë¡œë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

            except Exception as e:
                import traceback
                st.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                st.code(traceback.format_exc())
                
    # ëª¨ë¸ ìƒíƒœ í‘œì‹œ
    if st.session_state.get('model_loaded', False):
        st.success(" ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ì¶”ë¡  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        with st.expander("ëª¨ë¸ ì •ë³´"):
            model = st.session_state.model
            st.write(f"- ëª¨ë¸ íƒ€ì…: {model.config.model_type}")
            st.write(f"- ë¼ë²¨ ìˆ˜: {model.config.num_labels}")
            if hasattr(model.config, 'id2label') and model.config.id2label:
                st.write("- ë¼ë²¨ ë§¤í•‘:")
                for id_val, label in model.config.id2label.items():
                    st.write(f"  - {id_val}: {label}")
    else:
        st.warning(" ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")

    st.markdown("---")
    
    # ì¶”ë¡  ë°ì´í„° ì¤€ë¹„
    st.write("**ì¶”ë¡  ë°ì´í„° ì„¤ì •**")
    
    # ì¶”ë¡  ì‹¤í–‰
    if st.session_state.get('uploaded_df') is not None and st.session_state.get('model_loaded', False):
        df = st.session_state.uploaded_df
        
        st.write("ì‚¬ì´ë“œë°”ì—ì„œ ì—…ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        st.dataframe(df.head())
        st.info(f"ì´ {len(df)}ê°œì˜ í–‰ì´ ìˆìŠµë‹ˆë‹¤.")

        # ì¹¼ëŸ¼ ì„ íƒ
        text_column = st.selectbox(
            "ì¶”ë¡ ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼",
            df.columns.tolist(),
            key="inference_text_column"
        )

        if text_column:
            # ë°ì´í„° ì¤€ë¹„
            data_to_infer = df[text_column].dropna().astype(str).tolist()
            st.info(f" {len(data_to_infer)}ê°œì˜ ìƒ˜í”Œì´ ì¶”ë¡  ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì¶”ë¡  ì„¤ì •
            col1, col2 = st.columns(2)
            with col1:
                batch_size = st.number_input(
                    "ë°°ì¹˜ í¬ê¸°",
                    min_value=1,
                    max_value=32,
                    value=8,
                    step=1,
                    key="inference_batch_size"
                )
            with col2:
                max_length = st.number_input(
                    "ìµœëŒ€ í† í° ê¸¸ì´",
                    min_value=128,
                    max_value=1024,
                    value=256,
                    step=32,
                    key="max_length"
                )

            # ì¶”ë¡  ì‹¤í–‰ ë²„íŠ¼
            if st.button("ì¶”ë¡  ì‹¤í–‰", type="primary"):
                with st.spinner("ì¶”ë¡  ì‹¤í–‰ ì¤‘..."):
                    progress_bar = st.progress(0)
                    
                    model = st.session_state.model
                    tokenizer = st.session_state.tokenizer
                    device = next(model.parameters()).device
                    
                    # ì˜ˆì‹œ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                    preds = []
                    
                    for i in range(0, len(data_to_infer), batch_size):
                        batch_texts = data_to_infer[i : i + batch_size]
                        
                        try:
                            encodings = tokenizer(
                                batch_texts,
                                padding=True,
                                truncation=True,
                                max_length=max_length,
                                return_tensors="pt",
                            )
                            encodings = {k: v.to(device) for k, v in encodings.items()}

                            with torch.no_grad():
                                outputs = model(**encodings)
                                logits = outputs.logits
                                batch_preds = torch.argmax(logits, dim=-1).tolist()
                                preds.extend(batch_preds)
                                
                        except Exception as e:
                            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
                            for _ in batch_texts:
                                preds.append(f"ì˜¤ë¥˜: {str(e)}")

                        progress_bar.progress((i + len(batch_texts)) / len(data_to_infer))

                    # ê²°ê³¼ ì •ë¦¬
                    results = []
                    for j, text in enumerate(data_to_infer):
                        results.append({
                            'original_text': text,
                            'predicted_class': preds[j] if j < len(preds) else "ì˜¤ë¥˜",
                        })

                    st.session_state.inference_results = results
                    st.success(" ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.write(f"ì˜ˆì¸¡ ê²°ê³¼: {preds}")

    elif st.session_state.get('uploaded_df') is None:
        st.info("ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    elif not st.session_state.get('model_loaded', False):
        st.info("ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”.")

    # ì¶”ë¡  ê²°ê³¼ í‘œì‹œ
    if st.session_state.get('inference_results'):
        st.write(" **ì¶”ë¡  ê²°ê³¼**")
        results = st.session_state.inference_results
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        import pandas as pd
        results_df = pd.DataFrame(results)
        
        # ê²°ê³¼ ìš”ì•½
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì´ ìƒ˜í”Œ ìˆ˜", len(results_df))
        with col2:
            if 'predicted_class' in results_df.columns:
                unique_classes = results_df['predicted_class'].nunique()
                st.metric("ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ìˆ˜", unique_classes)
        
        # í´ë˜ìŠ¤ë³„ ë¶„í¬ í‘œì‹œ
        if 'predicted_class' in results_df.columns:
            st.write("**í´ë˜ìŠ¤ë³„ ë¶„í¬:**")
            class_counts = results_df['predicted_class'].value_counts()
            st.bar_chart(class_counts)
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
        st.write("**ìƒì„¸ ê²°ê³¼:**")
        st.dataframe(results_df)
        
        # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        csv = results_df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label=" ì¶”ë¡  ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="inference_results.csv",
            mime="text/csv"
        )
        
        # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©ëœ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        if st.session_state.get('uploaded_df') is not None:
            original_df = st.session_state.uploaded_df.copy()
            original_df['predicted_class'] = [r['predicted_class'] for r in results]
            
            combined_csv = original_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label=" ì›ë³¸+ì˜ˆì¸¡ê²°ê³¼ í†µí•© CSV ë‹¤ìš´ë¡œë“œ",
                data=combined_csv,
                file_name="combined_results.csv",
                mime="text/csv"
            )