{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "import os\n",
    "model_id='Qwen/Qwen3-0.6B'\n",
    "hf_token=\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side='right'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ],
   "id": "40a2530289a09d96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=5,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    token=hf_token  # 토큰 추가\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "e2a97f89ea7b0aae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 엑셀 데이터 로드\n",
    "# -------------------------------\n",
    "\n",
    "# 엑셀 파일 경로 (수정 필요)\n",
    "train_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/train.xlsx\"  # 200행\n",
    "test_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/test.xlsx\"    # 50행\n",
    "\n",
    "# 엑셀 읽기\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "print(f\"Train 데이터 크기: {train_df.shape}\")\n",
    "print(f\"Test 데이터 크기: {test_df.shape}\")\n",
    "print(f\"칼럼: {list(train_df.columns)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 텍스트 칼럼들을 합치기\n",
    "# -------------------------------\n",
    "\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    \"\"\"4개 텍스트 칼럼을 하나로 합치기\"\"\"\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():  # null이 아니고 빈 문자열이 아닌 경우\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "# 텍스트 칼럼 합치기\n",
    "train_df['text'] = train_df.apply(combine_text_columns, axis=1)\n",
    "test_df['text'] = test_df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "print(f\"\\n합쳐진 텍스트 예시:\")\n",
    "print(train_df['text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 라벨 인코딩\n",
    "# -------------------------------\n",
    "\n",
    "# 사용자태그를 라벨로 변환\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train + test 전체 라벨로 인코더 학습 (일관성 유지)\n",
    "all_labels = pd.concat([train_df['사용자태그'], test_df['사용자태그']])\n",
    "label_encoder.fit(all_labels.dropna())\n",
    "\n",
    "# 라벨 인코딩 적용\n",
    "train_df['label'] = label_encoder.transform(train_df['사용자태그'])\n",
    "test_df['label'] = label_encoder.transform(test_df['사용자태그'])\n",
    "\n",
    "# 라벨 정보 출력\n",
    "unique_labels = label_encoder.classes_\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"\\n라벨 정보:\")\n",
    "print(f\"총 라벨 수: {num_labels}\")\n",
    "for i, label in enumerate(unique_labels):\n",
    "    print(f\"  {i}: {label}\")\n",
    "\n",
    "print(f\"\\nTrain 라벨 분포:\")\n",
    "print(train_df['사용자태그'].value_counts())\n",
    "print(f\"\\nTest 라벨 분포:\")\n",
    "print(test_df['사용자태그'].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Dataset 객체로 변환\n",
    "# -------------------------------\n",
    "\n",
    "# 필요한 칼럼만 선택 (text, label)\n",
    "train_dataset_dict = {\n",
    "    'text': train_df['text'].tolist(),\n",
    "    'label': train_df['label'].tolist()\n",
    "}\n",
    "\n",
    "test_dataset_dict = {\n",
    "    'text': test_df['text'].tolist(),\n",
    "    'label': test_df['label'].tolist()\n",
    "}\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_data = Dataset.from_dict(train_dataset_dict)\n",
    "test_data = Dataset.from_dict(test_dataset_dict)\n",
    "\n",
    "# DatasetDict 생성\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': test_data\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset 생성 완료:\")\n",
    "print(f\"Train: {len(dataset['train'])}개\")\n",
    "print(f\"Test: {len(dataset['test'])}개\")\n"
   ],
   "id": "df26a1375d618bff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "    tokenized['labels'] = examples['label']\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train = dataset['train'].map(preprocess_function, batched=True)\n",
    "tokenized_test = dataset['test'].map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(['text', 'label'])\n",
    "tokenized_test = tokenized_test.remove_columns(['text', 'label'])\n",
    "\n",
    "print(f\"\\n토크나이징 완료:\")\n",
    "print(tokenized_train[0])"
   ],
   "id": "ad3f54cb12d26844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "def compute_metrics(pred):\n",
    "    labels=pred.label_ids\n",
    "    preds=pred.predictions.argmax(-1)\n",
    "    precision, recall, f1,_ =precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc=accuracy_score(labels,preds)\n",
    "    print(\"\\n Classification Report\")\n",
    "    print(classification_report(labels, preds, digits=2))\n",
    "    logits_tensor=torch.tensor(pred.predictions)\n",
    "    labels_tensor=torch.tensor(pred.label_ids)\n",
    "    loss=F.cross_entropy(logits_tensor,labels_tensor).item()\n",
    "    return{\n",
    "        'accuracy':acc,\n",
    "        'f1':f1,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'eval_loss':loss\n",
    "    }"
   ],
   "id": "399cf6ec3e08ea77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "peft_config=LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS',\n",
    "    target_modules=['k_proj','gate_proj','v_proj','up_proj','q_proj','o_proj','down_proj']\n",
    ")\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "model=get_peft_model(model,peft_config)"
   ],
   "id": "1485c0a38efe0bba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "output_dir= \"/output\"\n",
    "training_arguments=SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim='paged_adamw_32bit',\n",
    "    lr_scheduler_type='cosine',\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field='text',\n",
    "    max_length=512,\n",
    "    label_names=['labels']\n",
    ")"
   ],
   "id": "ea24041b2ebdd085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from trl import SFTTrainer\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "trainer.model.save_pretrained('Qwen3-0.6B-QLoRA2')"
   ],
   "id": "591d60b323e438d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------------\n",
    "# QLoRA SEQ_CLS 어댑터 머지\n",
    "# -------------------------------\n",
    "\n",
    "print(\"=== QLoRA Adapter Merge ===\")\n",
    "\n",
    "# 1. 베이스 모델을 SEQ_CLS로 직접 로드 (어댑터 훈련 시와 동일)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    num_labels=5,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "print(\"Base model structure:\")\n",
    "for name, _ in base_model.named_modules():\n",
    "    if 'score' in name or 'classifier' in name:\n",
    "        print(f\"  Found: {name}\")\n",
    "\n",
    "# 2. 어댑터 로드 및 머지\n",
    "adapter_path = \"/Qwen3-0.6B-QLoRA\"\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    ignore_mismatched_sizes=False,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# 3. 머지 및 저장\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"C:/wips/output/Qwen3_merged_method2\")\n",
    "print(\"머지 완료!\")"
   ],
   "id": "ec31f5f8e0de2c24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드 (기존 코드 그대로)\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 엑셀 데이터 로드 및 전처리\n",
    "# -------------------------------\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "train_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/train.xlsx\"  # 200행\n",
    "test_excel_path = \"C:/Users/nice.DESKTOP-0JCR5PF/Desktop/test.xlsx\"    # 50행\n",
    "\n",
    "# 엑셀 읽기\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "# 텍스트 칼럼들을 합치기\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "train_df['text'] = train_df.apply(combine_text_columns, axis=1)\n",
    "test_df['text'] = test_df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = pd.concat([train_df['사용자태그'], test_df['사용자태그']])\n",
    "label_encoder.fit(all_labels.dropna())\n",
    "\n",
    "train_df['label'] = label_encoder.transform(train_df['사용자태그'])\n",
    "test_df['label'] = label_encoder.transform(test_df['사용자태그'])\n",
    "\n",
    "# Dataset 객체로 변환\n",
    "train_data = Dataset.from_dict({\n",
    "    'text': train_df['text'].tolist(),\n",
    "    'label': train_df['label'].tolist()\n",
    "})\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    'text': test_df['text'].tolist(),\n",
    "    'label': test_df['label'].tolist()\n",
    "})\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 기존 방식과 동일한 샘플링\n",
    "# -------------------------------\n",
    "\n",
    "# 기존 코드와 동일한 구조\n",
    "ds = DatasetDict({'train': train_data, 'test': test_data})\n",
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]\n",
    "\n",
    "num_labels = len(set(train[\"label\"]))\n",
    "train_per_label = 200 // num_labels  # 전체 200개를 라벨별로 균등 분배\n",
    "test_per_label = 50 // num_labels    # 전체 50개를 라벨별로 균등 분배\n",
    "\n",
    "print(f\"라벨 수: {num_labels}\")\n",
    "print(f\"라벨별 train 샘플: {train_per_label}개\")\n",
    "print(f\"라벨별 test 샘플: {test_per_label}개\")\n",
    "\n",
    "def sample_per_label(dataset, per_label, seed=42):\n",
    "    labels = np.array(dataset[\"label\"])\n",
    "    picked = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for l in range(num_labels):\n",
    "        idx = np.where(labels == l)[0]\n",
    "        if len(idx) < per_label:\n",
    "            print(f\"경고: 라벨 {l}에 {len(idx)}개 샘플만 있음 (필요: {per_label}개)\")\n",
    "            picked.extend(idx)  # 있는 만큼만 추가\n",
    "        else:\n",
    "            picked.extend(rng.choice(idx, per_label, replace=False))\n",
    "    return dataset.select(sorted(picked))\n",
    "\n",
    "train_small = sample_per_label(train, train_per_label)\n",
    "test_small = sample_per_label(test, test_per_label)\n",
    "\n",
    "# 전체 데이터 (실제로는 train 200 + test 50 = 250개)\n",
    "all_texts = list(train_small[\"text\"]) + list(test_small[\"text\"])\n",
    "all_labels = list(train_small[\"label\"]) + list(test_small[\"label\"])\n",
    "\n",
    "print(f\"\\n전체 샘플: {len(all_texts)}개\")\n",
    "print(f\"Train 샘플: {len(train_small)}개\")\n",
    "print(f\"Test 샘플: {len(test_small)}개\")\n",
    "\n",
    "# 라벨 분포 확인\n",
    "from collections import Counter\n",
    "print(f\"\\nTrain 라벨 분포: {Counter(train_small['label'])}\")\n",
    "print(f\"Test 라벨 분포: {Counter(test_small['label'])}\")\n",
    "\n",
    "# 원본 라벨로 변환해서 확인\n",
    "train_original_labels = [label_encoder.inverse_transform([label])[0] for label in train_small['label']]\n",
    "test_original_labels = [label_encoder.inverse_transform([label])[0] for label in test_small['label']]\n",
    "\n",
    "print(f\"\\nTrain 원본 라벨 분포: {Counter(train_original_labels)}\")\n",
    "print(f\"Test 원본 라벨 분포: {Counter(test_original_labels)}\")\n",
    "\n",
    "# 모델 로드 (GPU 강제 사용)\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"사용할 디바이스: cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델 로드 (device_map 사용하지 않고 직접 이동)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/output/Qwen3_merged_method2\",\n",
    "    num_labels=5,\n",
    "    torch_dtype=torch.float16  # GPU 메모리 절약\n",
    ")\n",
    "\n",
    "# GPU로 강제 이동\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"모델 디바이스: {next(model.parameters()).device}\")\n",
    "print(f\"모델 dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# GPU 메모리 확인\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 예측\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(all_texts), 8):  # 배치 8\n",
    "        batch_texts = all_texts[i:i+8]\n",
    "        inputs = tokenizer(batch_texts, truncation=True, padding=True,\n",
    "                          max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# 결과\n",
    "accuracy = accuracy_score(all_labels, predictions)\n",
    "\n",
    "# 학습 데이터 vs 검증 데이터 비교\n",
    "train_acc = accuracy_score(train_small[\"label\"], predictions[:200])\n",
    "val_acc = accuracy_score(test_small[\"label\"], predictions[200:])\n"
   ],
   "id": "2eef77558bdc48b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a985abf09a27480",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
