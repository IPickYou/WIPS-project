{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID=\"Qwen/Qwen3-0.6B\"\n",
    "HF_TOKEN=\"\"\n",
    "\n",
    "TRAIN_PATH = \"E:/WIPS/workspace/dmc-review/train.xlsx\"  # 200행\n",
    "TEST_PATH = \"E:/WIPS/workspace/dmc-review/test.xlsx\"    # 50행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a54dc7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=5,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN  # 토큰 추가\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "153c5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    labels  patent_id  \\\n",
      "0  발명의 명칭: Method of producing zeolite film | 요약:...  CPC_C01B  16/762921   \n",
      "1  발명의 명칭: Process for preparing an IZM-2 zeolite...  CPC_C01B  17/032093   \n",
      "2  발명의 명칭: Liquid hydrogen storage material | 요약:...  CPC_C01B  16/193627   \n",
      "3  발명의 명칭: Preparation method of trifluoroamine o...  CPC_C01B  16/624752   \n",
      "4  발명의 명칭: Synthetic, multifaceted halogenated, f...  CPC_C01B  16/946892   \n",
      "\n",
      "   label_id  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "Index(['text', 'labels', 'label_id', 'patent_id'], dtype='object')\n",
      "                                                text    labels  label_id  \\\n",
      "0  발명의 명칭: Method of producing zeolite film | 요약:...  CPC_C01B         0   \n",
      "1   gel for growing the fine crystals;a third ste...  CPC_C01B         0   \n",
      "2  발명의 명칭: Process for preparing an IZM-2 zeolite...  CPC_C01B         0   \n",
      "3   Process for preparing an IZM-2 zeolite, compr...  CPC_C01B         0   \n",
      "4  ,BF/XO2 between 0 and 0.25,with X being one or...  CPC_C01B         0   \n",
      "\n",
      "   patent_id  \n",
      "0  16/762921  \n",
      "1  16/762921  \n",
      "2  17/032093  \n",
      "3  17/032093  \n",
      "4  17/032093  \n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 998/998 [00:01<00:00, 662.78 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 671.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_excel(TRAIN_PATH)\n",
    "test_df = pd.read_excel(TEST_PATH)\n",
    "\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    \"\"\"4개 텍스트 칼럼을 하나로 합치기\"\"\"\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():  # null이 아니고 빈 문자열이 아닌 경우\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "# 텍스트 칼럼 합치기\n",
    "df['text'] = df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "df = pd.DataFrame({\"text\": df[\"text\"], \"labels\": df[\"사용자태그\"], \"patent_id\": df[\"출원번호\"]})\n",
    "\n",
    "labels_list = sorted(df[\"labels\"].unique())\n",
    "label2id = {l: i for i, l in enumerate(labels_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[\"labels\"].map(label2id)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=512, stride=50):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunks.append(tokenizer.decode(tokens[start:end], skip_special_tokens=True))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start += max_length - stride\n",
    "    return chunks\n",
    "\n",
    "chunked_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row[\"text\"], tokenizer, max_length=512, stride=256)\n",
    "    for chunk in chunks:\n",
    "        chunked_rows.append({\n",
    "            \"text\": chunk,\n",
    "            \"labels\": row[\"labels\"],\n",
    "            \"label_id\": row[\"label_id\"],\n",
    "            \"patent_id\": row[\"patent_id\"]\n",
    "        })\n",
    "\n",
    "df_chunked = pd.DataFrame(chunked_rows)\n",
    "\n",
    "print(df_chunked.columns)\n",
    "print(df_chunked.head())\n",
    "print(df_chunked[\"label_id\"].isna().sum())\n",
    "\n",
    "train_df, eval_df = train_test_split(df_chunked, test_size=0.2, stratify=df_chunked['label_id'], random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    inputs = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = example[\"label_id\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=[\"labels\", \"label_id\", \"patent_id\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, remove_columns=[\"labels\", \"label_id\", \"patent_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "931d112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "def compute_metrics(pred):\n",
    "    labels=pred.label_ids\n",
    "    preds=pred.predictions.argmax(-1)\n",
    "    precision, recall, f1,_ =precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc=accuracy_score(labels,preds)\n",
    "    print(\"\\n Classification Report\")\n",
    "    print(classification_report(labels, preds, digits=2))\n",
    "    logits_tensor=torch.tensor(pred.predictions)\n",
    "    labels_tensor=torch.tensor(pred.label_ids)\n",
    "    loss=F.cross_entropy(logits_tensor,labels_tensor).item()\n",
    "    return{\n",
    "        'accuracy':acc,\n",
    "        'f1':f1,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'eval_loss':loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a684e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "peft_config=LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS',\n",
    "    target_modules=['k_proj','gate_proj','v_proj','up_proj','q_proj','o_proj','down_proj']\n",
    ")\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "model=get_peft_model(model,peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22e00875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "output_dir= \"../output\"\n",
    "training_arguments=SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim='paged_adamw_32bit',\n",
    "    lr_scheduler_type='cosine',\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field='text',\n",
    "    max_length=512,\n",
    "    label_names=['labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|██████████| 998/998 [00:04<00:00, 234.16 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 250/250 [00:00<00:00, 39026.95 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185/500 39:08 < 1:07:21, 0.08 it/s, Epoch 0.74/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.891600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.680400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from trl import SFTTrainer\n",
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "trainer.model.save_pretrained('Qwen3-0.6B-QLoRA2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f556a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QLoRA Adapter Merge ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model structure:\n",
      "  Found: score\n",
      "머지 완료!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------------\n",
    "# QLoRA SEQ_CLS 어댑터 머지\n",
    "# -------------------------------\n",
    "\n",
    "print(\"=== QLoRA Adapter Merge ===\")\n",
    "\n",
    "# 1. 베이스 모델을 SEQ_CLS로 직접 로드 (어댑터 훈련 시와 동일)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B\",\n",
    "    num_labels=5,\n",
    "    ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "print(\"Base model structure:\")\n",
    "for name, _ in base_model.named_modules():\n",
    "    if 'score' in name or 'classifier' in name:\n",
    "        print(f\"  Found: {name}\")\n",
    "\n",
    "# 2. 어댑터 로드 및 머지\n",
    "adapter_path = \"Qwen3-0.6B-QLoRA2\"\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    ignore_mismatched_sizes=False,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# 3. 머지 및 저장\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"C:/wips/output/Qwen3_merged_method3\")\n",
    "print(\"머지 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f9ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능: True\n",
      "GPU 이름: NVIDIA GeForce GTX 1660 Ti\n",
      "GPU 메모리: 6.00 GB\n",
      "현재 GPU 메모리 사용량: 1.08 GB\n",
      "토크나이저 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로딩 완료!\n",
      "모델 로딩 중...\n",
      "모델 로딩 완료!\n",
      "모델 디바이스: cuda:0\n",
      "모델 dtype: torch.float16\n",
      "GPU 메모리 사용량: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"현재 GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로딩 중...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"토크나이저 로딩 완료!\")\n",
    "\n",
    "# 모델 로드 (수정된 부분)\n",
    "print(\"모델 로딩 중...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    r\"C:/wips/output/Qwen3_merged_method3\",\n",
    "    num_labels=5,  # 동적으로 계산된 라벨 수 사용\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",  # 자동 디바이스 배치만 사용\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"모델 로딩 완료!\")\n",
    "\n",
    "# pad_token_id 설정\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"모델 디바이스: {next(model.parameters()).device}\")\n",
    "print(f\"모델 dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# GPU 메모리 사용량 확인\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55221e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    labels  patent_id  \\\n",
      "0  발명의 명칭: Method of producing zeolite film | 요약:...  CPC_C01B  16/762921   \n",
      "1  발명의 명칭: Process for preparing an IZM-2 zeolite...  CPC_C01B  17/032093   \n",
      "2  발명의 명칭: Liquid hydrogen storage material | 요약:...  CPC_C01B  16/193627   \n",
      "3  발명의 명칭: Preparation method of trifluoroamine o...  CPC_C01B  16/624752   \n",
      "4  발명의 명칭: Synthetic, multifaceted halogenated, f...  CPC_C01B  16/946892   \n",
      "\n",
      "   label_id  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "Index(['text', 'labels', 'label_id', 'patent_id'], dtype='object')\n",
      "                                                text    labels  label_id  \\\n",
      "0  발명의 명칭: Method of producing zeolite film | 요약:...  CPC_C01B         0   \n",
      "1  olite film is sequentially taken out of the co...  CPC_C01B         0   \n",
      "2  발명의 명칭: Process for preparing an IZM-2 zeolite...  CPC_C01B         0   \n",
      "3  O2 between 1 and 100,R(OH)2/XO2 between 0.006 ...  CPC_C01B         0   \n",
      "4  5. Process according to claim 1, in which BF i...  CPC_C01B         0   \n",
      "\n",
      "   patent_id  \n",
      "0  16/762921  \n",
      "1  16/762921  \n",
      "2  17/032093  \n",
      "3  17/032093  \n",
      "4  17/032093  \n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 693/693 [00:00<00:00, 698.48 examples/s]\n",
      "Map: 100%|██████████| 174/174 [00:00<00:00, 746.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트셋 로짓 계산 중...\n",
      "KeysView({'labels': tensor([2, 3, 3, 4, 4, 3, 4, 0]), 'label_id': tensor([2, 3, 3, 4, 4, 3, 4, 0]), '__index_level_0__': tensor([263, 826, 494, 622, 592, 813, 586,  71]), 'input_ids': tensor([[  6554,  44204,   4269,  ...,    264,  14409,   4982],\n",
      "        [   632,     11,    323,  ..., 151643, 151643, 151643],\n",
      "        [     8,    525,  11691,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [ 35741,  32237,  34619,  ...,  32237,  34619,  83343],\n",
      "        [126835,  79632,  20401,  ...,   3717,    220,     19],\n",
      "        [   220,     19,     13,  ...,     15,     15,  11616]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 1, 2, 2, 1, 0, 2, 4]), 'label_id': tensor([3, 1, 2, 2, 1, 0, 2, 4]), '__index_level_0__': tensor([503, 149, 265, 774, 174, 107, 410, 614]), 'input_ids': tensor([[  4929,     11,   2340,  ..., 151643, 151643, 151643],\n",
      "        [ 29845,    745,  18250,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,   3923,    759,  23609],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,  56807,   9317,  78006],\n",
      "        [  1156,   6819,   2670,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 4, 3, 2, 4, 2, 1, 4]), 'label_id': tensor([3, 4, 3, 2, 4, 2, 1, 4]), '__index_level_0__': tensor([456, 612, 824, 326, 624, 308, 134, 641]), 'input_ids': tensor([[    11,  24649,  24691,  ..., 151643, 151643, 151643],\n",
      "        [   220,     16,     11,  ...,     20,    595,     32],\n",
      "        [    16,     11,  41318,  ...,    374,    304,    279],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,  88318,  47373,    476],\n",
      "        [126835,  79632,  20401,  ...,   6396,   4379,   6461],\n",
      "        [    15,  67325,     27,  ...,  11169,     17,      8]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([1, 2, 0, 4, 3, 3, 1, 4]), 'label_id': tensor([1, 2, 0, 4, 3, 3, 1, 4]), '__index_level_0__': tensor([762, 345,  48, 606, 431, 477, 166, 589]), 'input_ids': tensor([[  2356,  16643,     87,  ...,     13,    576,   1882],\n",
      "        [ 98569,    438,   7707,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,   2088,    374,    911],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,  15261,  23609,  14011],\n",
      "        [126835,  79632,  20401,  ...,   6819,   1119,  90903],\n",
      "        [  1714,    315,   3717,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([1, 3, 0, 3, 4, 0, 1, 1]), 'label_id': tensor([1, 3, 0, 3, 4, 0, 1, 1]), '__index_level_0__': tensor([235, 837,  14, 819, 644, 103, 755, 256]), 'input_ids': tensor([[  1887,   5496,    311,  ...,  39995,    264,   1887],\n",
      "        [ 79107,  15590,  44412,  ..., 151643, 151643, 151643],\n",
      "        [  6144,    311,    220,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [    11,   3841,     18,  ...,   6291,    476,  85612],\n",
      "        [126835,  79632,  20401,  ...,     17,     13,    576],\n",
      "        [126835,  79632,  20401,  ...,   1156,   1482,    323]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 3, 3, 3, 1, 4, 2, 3]), 'label_id': tensor([3, 3, 3, 3, 1, 4, 2, 3]), '__index_level_0__': tensor([533, 557, 438, 505, 243, 618, 375, 825]), 'input_ids': tensor([[    22,     13,     23,  ...,     17,   4846,     26],\n",
      "        [   834,   1076,  50218,  ...,    311,    279,  30691],\n",
      "        [  1990,  16643,     87,  ...,  44662,   3539,     18],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,     13,     20,    311],\n",
      "        [  4510,   5778,     13,  ..., 151643, 151643, 151643],\n",
      "        [  1874,  30606,    315,  ...,    632,     11,    323]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([2, 2, 0, 3, 0, 1, 3, 0]), 'label_id': tensor([2, 2, 0, 3, 0, 1, 3, 0]), '__index_level_0__': tensor([291, 358,  58, 520, 734, 218, 449,  52]), 'input_ids': tensor([[  1119,    264,  14409,  ...,   4269,  12985,    304],\n",
      "        [     4,  61175,    323,  ..., 151643, 151643, 151643],\n",
      "        [   389,    264,   2790,  ...,    264,  24301,     11],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,   7956,   9317,  14511],\n",
      "        [126835,  79632,  20401,  ...,   1380,    356,  38760],\n",
      "        [  5395,  13479,   1824,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 0, 0, 1, 1, 1, 1, 3]), 'label_id': tensor([3, 0, 0, 1, 1, 1, 1, 3]), '__index_level_0__': tensor([455,  74, 121, 259, 223, 206, 756, 524]), 'input_ids': tensor([[   315,   1948,    220,  ...,    773,    438,    311],\n",
      "        [    16,     11,  41318,  ...,  44605,    973,     11],\n",
      "        [   323,   1694,  12678,  ...,     26,  11501,    287],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,    323,    603,    315],\n",
      "        [ 12720,  55329,     26,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,    315,    220,     20]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([1, 4, 0, 0, 2, 1, 0, 2]), 'label_id': tensor([1, 4, 0, 0, 2, 1, 0, 2]), '__index_level_0__': tensor([252, 845, 717,  63, 389, 214, 712, 294]), 'input_ids': tensor([[   323,    279,   1156,  ...,  11616,    356,     13],\n",
      "        [    27,     68,  11884,  ..., 144637,     69, 144637],\n",
      "        [126835,  79632,  20401,  ...,     13,    760,    220],\n",
      "        ...,\n",
      "        [    11,    320,   3808,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,  13851,    291,   7112],\n",
      "        [    25,  26625,   1053,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 3, 0, 1, 2, 0, 2, 3]), 'label_id': tensor([4, 3, 0, 1, 2, 0, 2, 3]), '__index_level_0__': tensor([674, 442,  99, 251, 782,  20, 419, 532]), 'input_ids': tensor([[    87,  33263,     19,  ...,      8,     46,     23],\n",
      "        [ 27672,     11,    264,  ...,    632,     11,   1853],\n",
      "        [  1156,  13740,  65081,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,   8123,   1786,    372],\n",
      "        [   220,     16,     17,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,     13,    760,    220]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([2, 4, 4, 4, 4, 3, 2, 0]), 'label_id': tensor([2, 4, 4, 4, 4, 3, 2, 0]), '__index_level_0__': tensor([317, 562, 688, 658, 642, 830, 383,  37]), 'input_ids': tensor([[ 24394,  30956,   2779,  ..., 151643, 151643, 151643],\n",
      "        [  2897,  43119,    553,  ..., 151643, 151643, 151643],\n",
      "        [  1255,   4737,  44883,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [  2268,   5514,  60833,  ...,    264,   2086,  60833],\n",
      "        [  4680,    389,   9058,  ...,   3090,     64,      8],\n",
      "        [    11,    279,   1714,  ...,    448,  46403,  18952]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 4, 2, 0, 3, 4, 0, 2]), 'label_id': tensor([4, 4, 2, 0, 3, 4, 0, 2]), '__index_level_0__': tensor([654, 692, 277,  54, 519, 661, 743, 792]), 'input_ids': tensor([[    13,    362,   1714,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,     16,   5265,    362],\n",
      "        [ 50558,     13,    760,  ..., 126524,     25,    220],\n",
      "        ...,\n",
      "        [   576,   1714,   4092,  ...,   5543,      8,    323],\n",
      "        [    19,     13,    576,  ..., 151643, 151643, 151643],\n",
      "        [  1053,  49584,  12896,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 2, 1, 0, 4, 0, 1, 0]), 'label_id': tensor([4, 2, 1, 0, 4, 0, 1, 0]), '__index_level_0__': tensor([585, 789, 125, 102, 570,  50, 203,  21]), 'input_ids': tensor([[   458,    458,    534,  ..., 151643, 151643, 151643],\n",
      "        [  3717,    220,     16,  ..., 151643, 151643, 151643],\n",
      "        [   264,  44901,   7132,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,   4623,  39995,    264],\n",
      "        [   458,  24510,  34684,  ...,     13,    576,  24394],\n",
      "        [   632,  10320,  25055,  ...,  13703,    337,    632]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 4, 3, 1, 2, 1, 2, 0]), 'label_id': tensor([3, 4, 3, 1, 2, 1, 2, 0]), '__index_level_0__': tensor([817, 838, 481, 238, 785, 750, 363, 106]), 'input_ids': tensor([[    17,     17,  32883,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,   2892,    295,    295],\n",
      "        [126835,  79632,  20401,  ...,    525,  31871,    553],\n",
      "        ...,\n",
      "        [  6291,    504,   3019,  ..., 151643, 151643, 151643],\n",
      "        [ 41286,   1053,  56807,  ...,   1882,    315,   3717],\n",
      "        [  1352,    315,    264,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([1, 3, 0, 1, 4, 2, 3, 1]), 'label_id': tensor([1, 3, 0, 1, 4, 2, 3, 1]), '__index_level_0__': tensor([173, 814, 711, 225, 664, 352, 515, 150]), 'input_ids': tensor([[ 38875,   6819,    553,  ...,  12896,    323,    264],\n",
      "        [ 34619,  83343,    374,  ..., 151643, 151643, 151643],\n",
      "        [  2101,     17,     46,  ..., 151643, 151643, 151643],\n",
      "        ...,\n",
      "        [  3245,    825,    949,  ..., 151643, 151643, 151643],\n",
      "        [  4531,  45238,     25,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,     23,     13,    576]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([3, 0, 3, 3, 4, 4, 1, 2]), 'label_id': tensor([3, 0, 3, 3, 4, 4, 1, 2]), '__index_level_0__': tensor([432,  88, 518, 440, 693, 582, 772, 283]), 'input_ids': tensor([[   825,    315,    279,  ..., 151643, 151643, 151643],\n",
      "        [    16,     11,  41318,  ..., 151643, 151643, 151643],\n",
      "        [   279,   2530,    315,  ...,   5517,   1677,    279],\n",
      "        ...,\n",
      "        [    16,     18,     11,  ...,  20021,    436,   3674],\n",
      "        [126835,  79632,  20401,  ...,  59983,    476,  59983],\n",
      "        [126835,  79632,  20401,  ...,    458,  15955,  12528]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([2, 2, 0, 1, 2, 1, 1, 2]), 'label_id': tensor([2, 2, 0, 1, 2, 1, 1, 2]), '__index_level_0__': tensor([798, 376, 116, 253, 313, 152, 230, 373]), 'input_ids': tensor([[  7434,    425,     17,  ...,    320,     85,  34594],\n",
      "        [126835,  79632,  20401,  ...,     19,     13,    576],\n",
      "        [   279,   1714,    374,  ...,     11,  41318,   2160],\n",
      "        ...,\n",
      "        [   220,     16,     19,  ..., 151643, 151643, 151643],\n",
      "        [  5263,    279,   1379,  ..., 151643, 151643, 151643],\n",
      "        [ 37131,   4462,   1660,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([2, 3, 2, 2, 2, 0, 3, 1]), 'label_id': tensor([2, 3, 2, 2, 2, 0, 3, 1]), '__index_level_0__': tensor([386, 531, 354, 417, 406,   1, 436, 181]), 'input_ids': tensor([[    17,     13,    760,  ...,     11,  45238,     25],\n",
      "        [ 48222,  43068,  19755,  ..., 151643, 151643, 151643],\n",
      "        [   311,   6851,  35741,  ...,  13621,     11,  16643],\n",
      "        ...,\n",
      "        [   337,    632,   4531,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,   4413,    278,    349],\n",
      "        [     8,   1660,  27802,  ..., 151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 4, 0, 2, 0, 4, 2, 1]), 'label_id': tensor([4, 4, 0, 2, 0, 4, 2, 1]), '__index_level_0__': tensor([668, 657,  57, 781, 730, 677, 387, 161]), 'input_ids': tensor([[126835,  79632,  20401,  ...,     65,  34295,    323],\n",
      "        [    16,     11,  41318,  ...,    831,    306,   9317],\n",
      "        [ 23552,   3042,    304,  ...,  16723,     26,  29795],\n",
      "        ...,\n",
      "        [126835,  79632,  20401,  ...,    315,  16643,  71867],\n",
      "        [   315,    279,  31620,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,    287,   3019,     13]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 3, 0, 0, 1, 4, 3, 0]), 'label_id': tensor([4, 3, 0, 0, 1, 4, 3, 0]), '__index_level_0__': tensor([564, 554,  87,  92, 250, 678, 539, 741]), 'input_ids': tensor([[   323,    294,  47444,  ...,     17,     15,     13],\n",
      "        [   296,     17,   4846,  ...,    287,    279,  12180],\n",
      "        [126835,  79632,  20401,  ...,    760,    220,     16],\n",
      "        ...,\n",
      "        [ 11660,    304,   3717,  ...,   3717,    220,     24],\n",
      "        [ 26359,  13621,   1788,  ...,  87086,  34619,  83343],\n",
      "        [126835,  79632,  20401,  ...,    760,    220,     20]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([4, 2, 2, 1, 3, 0, 1, 4]), 'label_id': tensor([4, 2, 2, 1, 3, 0, 1, 4]), '__index_level_0__': tensor([645, 276, 795, 240, 803, 738, 184, 569]), 'input_ids': tensor([[ 14122,     26,    323,  ..., 151643, 151643, 151643],\n",
      "        [126835,  79632,  20401,  ...,   3717,    220,     16],\n",
      "        [126835,  79632,  20401,  ...,    304,    220,     18],\n",
      "        ...,\n",
      "        [   220,     17,     13,  ...,   2268,   5730,  65696],\n",
      "        [   518,   3245,    825,  ...,  24394,  30956,   2779],\n",
      "        [ 39669,   4490,     13,  ...,     16,   6558,   2376]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "KeysView({'labels': tensor([1, 2, 2, 4, 2, 4]), 'label_id': tensor([1, 2, 2, 4, 2, 4]), '__index_level_0__': tensor([197, 305, 296, 615, 284, 565]), 'input_ids': tensor([[ 45238,     25,    458,  ...,    782,  90903,    504],\n",
      "        [   760,    220,     16,  ...,    220,     16,     20],\n",
      "        [   315,   1053,  56807,  ...,    264,   4168,    315],\n",
      "        [126835,  79632,  20401,  ..., 151643, 151643, 151643],\n",
      "        [     8,    323,  23415,  ...,    220,     17,     11],\n",
      "        [   458,    266,    519,  ...,  70304,  39489,   2048]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n",
      "torch.Size([6, 512])\n",
      "torch.Size([6])\n",
      "logits/probs 계산 완료: (174, 5)\n",
      "== Start evaluation ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CPC_C01B     0.4857    0.6800    0.5667        25\n",
      "    CPC_C01C     0.9167    0.8462    0.8800        26\n",
      "    CPC_C01D     0.6923    0.5806    0.6316        31\n",
      "    CPC_C01F     0.6667    0.4800    0.5581        25\n",
      "    CPC_C01G     0.5484    0.6296    0.5862        27\n",
      "\n",
      "    accuracy                         0.6418       134\n",
      "   macro avg     0.6619    0.6433    0.6445       134\n",
      "weighted avg     0.6635    0.6418    0.6448       134\n",
      "\n",
      "Accuracy: 0.6418, F1: 0.6448, Precision: 0.6635, Recall: 0.6418\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "function_schema = {\n",
    "    \"name\": \"classify_patent_claim\",\n",
    "    \"description\": \"특허 청구항을 CPC 카테고리로 분류합니다.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"pred_label\": {\"type\": \"string\", \"description\": \"예측된 CPC 코드\"},\n",
    "            \"reason\": {\"type\": \"object\", \"description\": \"예측 확률\"}\n",
    "        },\n",
    "        \"required\": [\"pred_label\", \"reason\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "train_df = pd.read_excel(TRAIN_PATH)\n",
    "test_df = pd.read_excel(TEST_PATH)\n",
    "\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "text_columns = ['발명의 명칭', '요약', '전체청구항', '대표청구항']\n",
    "\n",
    "def combine_text_columns(row):\n",
    "    \"\"\"4개 텍스트 칼럼을 하나로 합치기\"\"\"\n",
    "    texts = []\n",
    "    for col in text_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():  # null이 아니고 빈 문자열이 아닌 경우\n",
    "            texts.append(f\"{col}: {str(row[col]).strip()}\")\n",
    "    return \" | \".join(texts)\n",
    "\n",
    "# 텍스트 칼럼 합치기\n",
    "df['text'] = df.apply(combine_text_columns, axis=1)\n",
    "\n",
    "df = pd.DataFrame({\"text\": df[\"text\"], \"labels\": df[\"사용자태그\"], \"patent_id\": df[\"출원번호\"]})\n",
    "\n",
    "labels_list = sorted(df[\"labels\"].unique())\n",
    "label2id = {l: i for i, l in enumerate(labels_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[\"labels\"].map(label2id)\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=512, stride=50):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunks.append(tokenizer.decode(tokens[start:end], skip_special_tokens=True))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start += max_length - stride\n",
    "    return chunks\n",
    "\n",
    "chunked_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row[\"text\"], tokenizer, max_length=512, stride=256)\n",
    "    for chunk in chunks:\n",
    "        chunked_rows.append({\n",
    "            \"text\": chunk,\n",
    "            \"labels\": row[\"labels\"],\n",
    "            \"label_id\": row[\"label_id\"],\n",
    "            \"patent_id\": row[\"patent_id\"]\n",
    "        })\n",
    "\n",
    "df_chunked = pd.DataFrame(chunked_rows)\n",
    "\n",
    "print(df_chunked.columns)\n",
    "print(df_chunked.head())\n",
    "print(df_chunked[\"label_id\"].isna().sum())\n",
    "\n",
    "train_df, eval_df = train_test_split(df_chunked, test_size=0.2, stratify=df_chunked['label_id'], random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    enc = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    enc[\"labels\"] = example[\"label_id\"]\n",
    "    return enc\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, remove_columns=[\"labels\", \"label_id\", \"patent_id\"])\n",
    "remove_cols = [c for c in [\"text\", \"patent_id\"] if c in eval_dataset.column_names]\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, remove_columns=remove_cols)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "test_loader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# 2. 모델 예측 → probs 만들기\n",
    "model.eval()\n",
    "all_logits = []\n",
    "\n",
    "print(\"테스트셋 로짓 계산 중...\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        print(batch.keys())\n",
    "        print(batch['input_ids'].shape)\n",
    "        print(batch['labels'].shape)\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**batch)\n",
    "        all_logits.append(outputs.logits.cpu())\n",
    "\n",
    "logits = torch.cat(all_logits, dim=0)\n",
    "probs = torch.softmax(logits, dim=-1).numpy()\n",
    "\n",
    "print(\"logits/probs 계산 완료:\", probs.shape)\n",
    "\n",
    "# 3. 이후 코드는 기존과 동일\n",
    "eval_df = eval_df.reset_index(drop=True)\n",
    "eval_df['chunk_index'] = range(len(eval_df))\n",
    "eval_df['chunk_len'] = eval_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
    "\n",
    "patent_results = []\n",
    "for patent_id, group in eval_df.groupby('patent_id'):\n",
    "    indices = group['chunk_index'].tolist()\n",
    "    weights = group['chunk_len'].values\n",
    "    \n",
    "    chunk_probs = probs[indices]\n",
    "    mean_prob = chunk_probs.max(axis=0)\n",
    "    \n",
    "    pred_idx = mean_prob.argmax()\n",
    "    pred_label = id2label[pred_idx]\n",
    "    reason = {id2label[i]: float(mean_prob[i]) for i in range(len(mean_prob))}\n",
    "    \n",
    "    function_call = {\n",
    "        \"name\": function_schema[\"name\"],\n",
    "        \"arguments\": json.dumps({\n",
    "            \"pred_label\": pred_label,\n",
    "            \"reason\": reason\n",
    "        })\n",
    "    }\n",
    "    args = json.loads(function_call[\"arguments\"])\n",
    "    \n",
    "    patent_results.append({\n",
    "        \"patent_id\": patent_id,\n",
    "        \"pred_label\": args[\"pred_label\"],\n",
    "        \"reason\": args[\"reason\"],\n",
    "        \"true_label\": group['labels'].iloc[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(patent_results)\n",
    "\n",
    "# 4. 평가\n",
    "true_labels = results_df[\"true_label\"].tolist()\n",
    "pred_labels = results_df[\"pred_label\"].tolist()\n",
    "\n",
    "print(\"== Start evaluation ==\")\n",
    "print(classification_report(true_labels, pred_labels, digits=4))\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average=\"weighted\")\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_conet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
